{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Como instalar Pyspark no Google Colab",
      "provenance": [],
      "authorship_tag": "ABX9TyMc3NGm4ql1U1PqangTji4p"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tutorial simples e direto ensinando a instalar as dependências e a biblioteca.\n",
        "\n",
        "Instalando o PySpark no Google Colab\n",
        "Instalar o PySpark não é um processo direto como de praxe em Python. Não basta usar um pip install apenas. Na verdade, antes de tudo é necessário instalar dependências como o Java 8, Apache Spark 2.3.2 junto com o Hadoop 2.7."
      ],
      "metadata": {
        "id": "TaNNrvAE8d-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "VH8YDUJN7pZx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A próxima etapa é configurar as variáveis de ambiente, pois isso habilita o ambiente do Colab a identificar corretamente onde as dependências estão rodando.\n",
        "\n",
        "Para conseguir “manipular” o terminal e interagir como ele, você pode usar a biblioteca os."
      ],
      "metadata": {
        "id": "WnIhp8dt8zmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "metadata": {
        "id": "O4ujSXvx73vI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com tudo pronto, vamos rodar uma sessão local para testar se a instalação funcionou corretamente."
      ],
      "metadata": {
        "id": "Pl8stUGQ9Ib8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iniciar uma sessão local e importar dados do Airbnb\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# download do http para arquivo local\n",
        "!wget --quiet --show-progress http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2019-07-15/visualisations/listings.csv\n",
        "\n",
        "# carregar dados do Airbnb\n",
        "df_spark = sc.read.csv(\"./listings.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# ver algumas informações sobre os tipos de dados de cada coluna\n",
        "df_spark.printSchema()"
      ],
      "metadata": {
        "id": "EoF2suik9JIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}